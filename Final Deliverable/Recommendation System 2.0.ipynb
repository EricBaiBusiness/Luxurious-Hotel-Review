{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation System 1.0 (Current status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading & Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Original Dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opendatasets in /Users/hakukazuho/anaconda3/lib/python3.11/site-packages (0.1.22)\n",
      "Requirement already satisfied: tqdm in /Users/hakukazuho/anaconda3/lib/python3.11/site-packages (from opendatasets) (4.65.0)\n",
      "Requirement already satisfied: kaggle in /Users/hakukazuho/anaconda3/lib/python3.11/site-packages (from opendatasets) (1.6.4)\n",
      "Requirement already satisfied: click in /Users/hakukazuho/anaconda3/lib/python3.11/site-packages (from opendatasets) (8.0.4)\n",
      "Requirement already satisfied: six>=1.10 in /Users/hakukazuho/anaconda3/lib/python3.11/site-packages (from kaggle->opendatasets) (1.16.0)\n",
      "Requirement already satisfied: certifi in /Users/hakukazuho/anaconda3/lib/python3.11/site-packages (from kaggle->opendatasets) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil in /Users/hakukazuho/anaconda3/lib/python3.11/site-packages (from kaggle->opendatasets) (2.8.2)\n",
      "Requirement already satisfied: requests in /Users/hakukazuho/anaconda3/lib/python3.11/site-packages (from kaggle->opendatasets) (2.31.0)\n",
      "Requirement already satisfied: python-slugify in /Users/hakukazuho/anaconda3/lib/python3.11/site-packages (from kaggle->opendatasets) (5.0.2)\n",
      "Requirement already satisfied: urllib3 in /Users/hakukazuho/anaconda3/lib/python3.11/site-packages (from kaggle->opendatasets) (1.26.16)\n",
      "Requirement already satisfied: bleach in /Users/hakukazuho/anaconda3/lib/python3.11/site-packages (from kaggle->opendatasets) (4.1.0)\n",
      "Requirement already satisfied: packaging in /Users/hakukazuho/anaconda3/lib/python3.11/site-packages (from bleach->kaggle->opendatasets) (23.0)\n",
      "Requirement already satisfied: webencodings in /Users/hakukazuho/anaconda3/lib/python3.11/site-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /Users/hakukazuho/anaconda3/lib/python3.11/site-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/hakukazuho/anaconda3/lib/python3.11/site-packages (from requests->kaggle->opendatasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/hakukazuho/anaconda3/lib/python3.11/site-packages (from requests->kaggle->opendatasets) (3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install opendatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \"./515k-hotel-reviews-data-in-europe\" (use force=True to force download)\n"
     ]
    }
   ],
   "source": [
    "# Eric's Kaggle API key\n",
    "# \"username\":\"ericthedataguy\",\n",
    "# \"key\":\"875c0de0faea2fefa081c78eb470a347\"\n",
    "import opendatasets as od\n",
    "import pandas as pd\n",
    "\n",
    "od.download(\n",
    "    \"https://www.kaggle.com/datasets/jiashenliu/515k-hotel-reviews-data-in-europe\")\n",
    "\n",
    "df = pd.read_csv(\"515k-hotel-reviews-data-in-europe/Hotel_Reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in geographical data using Google API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import GoogleV3\n",
    "import pandas as pd\n",
    "missing_geo_hotel = list(df[df['lat'].isna()]['Hotel_Name'].value_counts().reset_index()['Hotel_Name'])\n",
    "missing_geo_address = list(df[df['lat'].isna()]['Hotel_Address'].value_counts().reset_index()['Hotel_Address'])\n",
    "\n",
    "# Create a geocoder object\n",
    "geolocator = GoogleV3(api_key='AIzaSyCo0MJ4SypoxliSIn-yyNG4F_eCFncRXoU')\n",
    "\n",
    "# Define a function to get the latitude and longitude of an address\n",
    "def get_coordinates(address):\n",
    "    location = geolocator.geocode(address)\n",
    "    if location:\n",
    "        return location.latitude, location.longitude\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Get the coordinates for each address in missing_geo_address\n",
    "coordinates = [get_coordinates(address) for address in missing_geo_address]\n",
    "\n",
    "# Create a new dataframe with hotel name, longitude, and latitude\n",
    "new_df = pd.DataFrame({'Hotel_Name': missing_geo_hotel, 'Longitude': [coord[1] if coord else None for coord in coordinates], 'Latitude': [coord[0] if coord else None for coord in coordinates]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 515738 entries, 0 to 515737\n",
      "Data columns (total 17 columns):\n",
      " #   Column                                      Non-Null Count   Dtype  \n",
      "---  ------                                      --------------   -----  \n",
      " 0   Hotel_Address                               515738 non-null  object \n",
      " 1   Additional_Number_of_Scoring                515738 non-null  int64  \n",
      " 2   Review_Date                                 515738 non-null  object \n",
      " 3   Average_Score                               515738 non-null  float64\n",
      " 4   Hotel_Name                                  515738 non-null  object \n",
      " 5   Reviewer_Nationality                        515738 non-null  object \n",
      " 6   Negative_Review                             515738 non-null  object \n",
      " 7   Review_Total_Negative_Word_Counts           515738 non-null  int64  \n",
      " 8   Total_Number_of_Reviews                     515738 non-null  int64  \n",
      " 9   Positive_Review                             515738 non-null  object \n",
      " 10  Review_Total_Positive_Word_Counts           515738 non-null  int64  \n",
      " 11  Total_Number_of_Reviews_Reviewer_Has_Given  515738 non-null  int64  \n",
      " 12  Reviewer_Score                              515738 non-null  float64\n",
      " 13  Tags                                        515738 non-null  object \n",
      " 14  days_since_review                           515738 non-null  object \n",
      " 15  lat                                         512470 non-null  float64\n",
      " 16  lng                                         512470 non-null  float64\n",
      "dtypes: float64(4), int64(5), object(8)\n",
      "memory usage: 66.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure both key columns are of the same data type, here assuming 'Hotel_Name' column exists in `df`\n",
    "df['Hotel_Name'] = df['Hotel_Name'].astype(str)  # Adjust if 'Hotel_Name' is the actual column you want to merge on in `df`\n",
    "new_df['Hotel_Name'] = new_df['Hotel_Name'].astype(str)\n",
    "\n",
    "# Merge using columns instead of trying to merge an index with a column\n",
    "df_filled = df.merge(new_df, left_on='Hotel_Name', right_on='Hotel_Name', how='left')\n",
    "df_filled['Latitude'] = df_filled['Latitude'].fillna(df_filled['lat'])\n",
    "df_filled['Longitude'] = df_filled['Longitude'].fillna(df_filled['lng'])\n",
    "df_filled = df_filled.drop(columns=['lat', 'lng'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data for Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting random 10k rows (1729)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Additional_Number_of_Scoring</th>\n",
       "      <th>Average_Score</th>\n",
       "      <th>Review_Total_Negative_Word_Counts</th>\n",
       "      <th>Total_Number_of_Reviews</th>\n",
       "      <th>Review_Total_Positive_Word_Counts</th>\n",
       "      <th>Total_Number_of_Reviews_Reviewer_Has_Given</th>\n",
       "      <th>Reviewer_Score</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.00000</td>\n",
       "      <td>10000.00000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>497.128300</td>\n",
       "      <td>8.408580</td>\n",
       "      <td>18.49140</td>\n",
       "      <td>2717.18750</td>\n",
       "      <td>17.706600</td>\n",
       "      <td>7.225500</td>\n",
       "      <td>8.386380</td>\n",
       "      <td>2.899397</td>\n",
       "      <td>49.450665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>509.107355</td>\n",
       "      <td>0.545186</td>\n",
       "      <td>30.42837</td>\n",
       "      <td>2330.16349</td>\n",
       "      <td>22.248506</td>\n",
       "      <td>11.131291</td>\n",
       "      <td>1.644268</td>\n",
       "      <td>4.691857</td>\n",
       "      <td>3.442442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>54.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>-0.369758</td>\n",
       "      <td>41.328376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>168.000000</td>\n",
       "      <td>8.100000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1145.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>-0.142745</td>\n",
       "      <td>48.214662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>337.000000</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>9.00000</td>\n",
       "      <td>2061.00000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>0.019886</td>\n",
       "      <td>51.499981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>639.000000</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>23.00000</td>\n",
       "      <td>3598.00000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.600000</td>\n",
       "      <td>4.834975</td>\n",
       "      <td>51.516048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2682.000000</td>\n",
       "      <td>9.600000</td>\n",
       "      <td>386.00000</td>\n",
       "      <td>16670.00000</td>\n",
       "      <td>367.000000</td>\n",
       "      <td>355.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>16.445799</td>\n",
       "      <td>52.400181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Additional_Number_of_Scoring  Average_Score  \\\n",
       "count                  10000.000000   10000.000000   \n",
       "mean                     497.128300       8.408580   \n",
       "std                      509.107355       0.545186   \n",
       "min                        6.000000       6.400000   \n",
       "25%                      168.000000       8.100000   \n",
       "50%                      337.000000       8.500000   \n",
       "75%                      639.000000       8.800000   \n",
       "max                     2682.000000       9.600000   \n",
       "\n",
       "       Review_Total_Negative_Word_Counts  Total_Number_of_Reviews  \\\n",
       "count                        10000.00000              10000.00000   \n",
       "mean                            18.49140               2717.18750   \n",
       "std                             30.42837               2330.16349   \n",
       "min                              0.00000                 54.00000   \n",
       "25%                              0.00000               1145.00000   \n",
       "50%                              9.00000               2061.00000   \n",
       "75%                             23.00000               3598.00000   \n",
       "max                            386.00000              16670.00000   \n",
       "\n",
       "       Review_Total_Positive_Word_Counts  \\\n",
       "count                       10000.000000   \n",
       "mean                           17.706600   \n",
       "std                            22.248506   \n",
       "min                             0.000000   \n",
       "25%                             5.000000   \n",
       "50%                            11.000000   \n",
       "75%                            22.000000   \n",
       "max                           367.000000   \n",
       "\n",
       "       Total_Number_of_Reviews_Reviewer_Has_Given  Reviewer_Score  \\\n",
       "count                                10000.000000    10000.000000   \n",
       "mean                                     7.225500        8.386380   \n",
       "std                                     11.131291        1.644268   \n",
       "min                                      1.000000        2.500000   \n",
       "25%                                      1.000000        7.500000   \n",
       "50%                                      3.000000        8.800000   \n",
       "75%                                      8.000000        9.600000   \n",
       "max                                    355.000000       10.000000   \n",
       "\n",
       "          Longitude      Latitude  \n",
       "count  10000.000000  10000.000000  \n",
       "mean       2.899397     49.450665  \n",
       "std        4.691857      3.442442  \n",
       "min       -0.369758     41.328376  \n",
       "25%       -0.142745     48.214662  \n",
       "50%        0.019886     51.499981  \n",
       "75%        4.834975     51.516048  \n",
       "max       16.445799     52.400181  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = df_filled.sample(n=10000, random_state=1729)\n",
    "subset.to_csv('10k_subset.csv', index=False)\n",
    "subset = pd.read_csv('10k_subset.csv')\n",
    "subset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract all tags from Positive_Review for each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Function to extract keywords/tags from a positive review\n",
    "def extract_keywords(review):\n",
    "    tokens = nltk.word_tokenize(review)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    keywords = []\n",
    "    noun = None\n",
    "    for word, pos in tagged_tokens:\n",
    "        if pos.startswith('NN'):  # Look for nouns\n",
    "            noun = word\n",
    "        elif pos.startswith('JJ') and noun:  # Look for adjectives describing nouns\n",
    "            keywords.append(f\"{word} {noun}\")\n",
    "            noun = None\n",
    "    return keywords\n",
    "\n",
    "subset['Keywords'] = subset['Positive_Review'].apply(extract_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pool tags for each hotel & add hotel name (groupby hotel name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the noun from a keyword combo of adjective + noun\n",
    "def extract_noun(keyword):\n",
    "    tokens = nltk.word_tokenize(keyword)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    nouns = [word for word, pos in tagged_tokens if pos.startswith('NN')]\n",
    "    return nouns[0] if nouns else None\n",
    "\n",
    "\n",
    "subset['text_tags'] = subset['Keywords'].apply(lambda keywords: list(set([extract_noun(keyword) for keyword in keywords])))\n",
    "subset['text_tags'] = subset['text_tags'].apply(lambda tags: [tag.capitalize() if tag is not None else None for tag in tags])\n",
    "\n",
    "subset['transactions'] = subset.apply(lambda row: [row['Hotel_Name']] + row['text_tags'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter & keep the most popular 10 tags for each hotel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the top 10 tags for each hotel\n",
    "from collections import Counter\n",
    "\n",
    "table1 = subset.groupby('Hotel_Name')['transactions'].sum().reset_index()\n",
    "\n",
    "hotel_lists = {}\n",
    "\n",
    "for hotel_name, transactions in table1.groupby('Hotel_Name')['transactions']:\n",
    "    transaction_list = transactions.tolist()\n",
    "    flattened_list = [item for sublist in transaction_list for item in sublist]\n",
    "    counter = Counter(flattened_list)\n",
    "    top_50_elements = [element for element, count in counter.most_common(10)]\n",
    "    hotel_lists[hotel_name] = top_50_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tags = pd.DataFrame({\n",
    "    'Hotel_Name': list(hotel_lists.keys()),\n",
    "    'Top_Tags': list(hotel_lists.values())\n",
    "})\n",
    "\n",
    "merged_table = subset.merge(top_tags, on='Hotel_Name')\n",
    "\n",
    "# Check each item in the list in transactions and add to a new list if it's in the list in Top_Tags\n",
    "merged_table['new_transactions'] = merged_table.apply(lambda row: [item for item in row['transactions'] if item in row['Top_Tags']], axis=1)\n",
    "merged_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conduct Market Basket Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "\n",
    "# apply transactionencoder\n",
    "te = TransactionEncoder()\n",
    "merged_table['new_transactions'] = merged_table['new_transactions'].apply(lambda x: [item for item in x if item is not None])\n",
    "te_ary = te.fit(merged_table['new_transactions']).transform(merged_table['new_transactions'])\n",
    "df_te = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# create a unique list of hotel name\n",
    "antecedents_hotel = merged_table['Hotel_Name'].unique()\n",
    "\n",
    "# apply Apriori algorithm\n",
    "frequent_itemsets = apriori(df_te, min_support=0.0000001, use_colnames=True)\n",
    "\n",
    "# Generate association rules\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.1)\n",
    "rules['antecedents'] = rules['antecedents'].apply(lambda x: set(x))\n",
    "rules['consequents'] = rules['consequents'].apply(lambda x: set(x))\n",
    "\n",
    "# filter rules\n",
    "single_antecedent_consequent_rules = rules[(rules['antecedents'].map(len) == 1) &\n",
    "                                           (rules['consequents'].map(len) == 1) &\n",
    "                                           (rules['antecedents'].apply(lambda x: list(x)[0]).isin(antecedents_hotel))]\n",
    "\n",
    "rules_table = single_antecedent_consequent_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that returns most popular tags for given hotel name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that finds the most popular tags of chosen hotel\n",
    "\n",
    "def popular_tags(hotel_name):\n",
    "    print(rules_table[rules_table['antecedents'] == {hotel_name}]['consequents'])\n",
    "\n",
    "popular_tags('Hotel Arena')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation System 2.0 (Improvement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan for improvement\n",
    "\n",
    "**Better text-to-tags results**\n",
    "\n",
    "1. Try n-grams\n",
    "\n",
    "2. Try TF-IDF\n",
    "\n",
    "3. Try Sentiment Analysis\n",
    "\n",
    "**Better user experiences**\n",
    "\n",
    "1. Word cloud with dropdown\n",
    "\n",
    "2. Input tags -> output hotel name\n",
    "\n",
    "**Other issues:**\n",
    "\n",
    "1. Subset using random seed\n",
    "\n",
    "2. Adjust rules\n",
    "\n",
    "3. Choose top tags first, then add hotel name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better text-tags results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Current version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Function to extract keywords/tags from a positive review\n",
    "def extract_keywords(review):\n",
    "    tokens = nltk.word_tokenize(review)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    keywords = []\n",
    "    noun = None\n",
    "    for word, pos in tagged_tokens:\n",
    "        if pos.startswith('NN'):  # Look for nouns\n",
    "            noun = word\n",
    "        elif pos.startswith('JJ') and noun:  # Look for adjectives describing nouns\n",
    "            keywords.append(f\"{word} {noun}\")\n",
    "            noun = None\n",
    "    return keywords\n",
    "\n",
    "subset['Keywords'] = subset['Positive_Review'].apply(extract_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/hakukazuho/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/hakukazuho/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/hakukazuho/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hakukazuho/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('staff', 0.5773502691896258),\n",
       " ('chatty', 0.2886751345948129),\n",
       " ('friendly', 0.2886751345948129),\n",
       " ('funny', 0.2886751345948129),\n",
       " ('group', 0.2886751345948129),\n",
       " ('helpful', 0.2886751345948129),\n",
       " ('hostile', 0.2886751345948129),\n",
       " ('hotel', 0.2886751345948129),\n",
       " ('young', 0.2886751345948129)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag, download\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure required NLTK resources are downloaded\n",
    "download('punkt')\n",
    "download('averaged_perceptron_tagger')\n",
    "download('wordnet')\n",
    "download('stopwords')\n",
    "\n",
    "# Review text\n",
    "review_text = \n",
    "\n",
    "# Preprocessing: Lowercasing\n",
    "review_text = review_text.lower()\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(review_text)\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# POS Tagging\n",
    "tagged_tokens = pos_tag(lemmatized_tokens)\n",
    "\n",
    "# Extracting Nouns and Adjectives\n",
    "nouns_and_adjectives = [token for token, tag in tagged_tokens if tag.startswith('NN') or tag.startswith('JJ')]\n",
    "\n",
    "# Applying TF-IDF on extracted nouns and adjectives\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform([' '.join(nouns_and_adjectives)])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Converting TF-IDF result to a readable format\n",
    "tfidf_scores = tfidf_matrix.toarray().flatten()\n",
    "tfidf_scores_dict = dict(zip(feature_names, tfidf_scores))\n",
    "\n",
    "# Sorting words by their TF-IDF scores\n",
    "sorted_tfidf = sorted(tfidf_scores_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "sorted_tfidf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
